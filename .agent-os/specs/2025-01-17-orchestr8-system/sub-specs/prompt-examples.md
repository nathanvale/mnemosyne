# Prompt Examples

This document provides concrete examples of @orchestr8 agent prompts demonstrating JSON-based prompt configuration (per ADR-002) with the scoped XML exception for the single MVP research agent.

> Created: 2025-01-17  
> Version: 1.0.0  
> Status: MVP Examples

## JSON Prompt Examples (Standard)

### Example 1: Basic Hello World Agent

```json
{
  "id": "hello-world-001",
  "version": "1.0.0",
  "name": "Hello World Agent",
  "description": "Simple demonstration agent with minimal complexity",
  "category": "demo",

  "structure": {
    "role": {
      "identity": "greeting-agent",
      "purpose": "Generate friendly greetings",
      "capabilities": {
        "primary": ["text_generation", "greeting_creation"],
        "constraints": ["no_personal_data", "family_friendly"]
      }
    },

    "instructions": {
      "steps": [
        {
          "id": "step-1",
          "action": "analyze_input",
          "description": "Parse the user's name from input",
          "validation": {
            "required": ["input.name"],
            "format": "string"
          }
        },
        {
          "id": "step-2",
          "action": "generate_greeting",
          "description": "Create personalized greeting message",
          "template": "Hello, {{name}}! Welcome to @orchestr8."
        }
      ]
    },

    "constraints": {
      "forbidden_behaviors": [
        "collecting_personal_information",
        "making_assumptions_about_user",
        "using_inappropriate_language"
      ],
      "required_behaviors": [
        "maintain_friendly_tone",
        "respect_user_preferences",
        "provide_clear_responses"
      ]
    },

    "output_format": {
      "type": "object",
      "properties": {
        "greeting": { "type": "string" },
        "timestamp": { "type": "string", "format": "date-time" },
        "agent_id": { "type": "string" }
      },
      "required": ["greeting", "timestamp", "agent_id"]
    },

    "error_handling": {
      "missing_input": {
        "action": "return_default",
        "default_response": "Hello! Welcome to @orchestr8."
      },
      "invalid_format": {
        "action": "log_and_retry",
        "max_retries": 2
      }
    }
  },

  "variables": [
    {
      "name": "name",
      "type": "string",
      "required": false,
      "default": "Friend",
      "description": "User's name for personalization",
      "validation": "^[a-zA-Z ]{1,50}$"
    }
  ],

  "quality_baseline": {
    "adherence_target": 95,
    "consistency_target": 0.98,
    "performance_target": 500
  },

  "metadata": {
    "created_at": "2025-01-17T00:00:00Z",
    "author": "system",
    "tags": ["demo", "simple", "greeting"],
    "usage_count": 0,
    "average_quality_score": null
  }
}
```

### Example 2: Changelog Generation Agent

```json
{
  "id": "changelog-generator-001",
  "version": "1.0.0",
  "name": "Changelog Generation Agent",
  "description": "Analyzes git commits and generates structured changelogs",
  "category": "analysis",

  "structure": {
    "role": {
      "identity": "changelog-analyst",
      "purpose": "Generate clear, categorized changelogs from commit history",
      "capabilities": {
        "primary": [
          "git_log_analysis",
          "semantic_versioning",
          "change_categorization",
          "markdown_generation"
        ],
        "constraints": [
          "no_code_execution",
          "read_only_operations",
          "respect_conventional_commits"
        ]
      },
      "expertise": {
        "domains": ["software_development", "version_control", "documentation"],
        "knowledge_base": [
          "conventional_commits",
          "semantic_versioning",
          "keep_a_changelog"
        ]
      }
    },

    "instructions": {
      "workflow": {
        "type": "sequential",
        "steps": [
          {
            "id": "fetch-commits",
            "action": "retrieve_git_log",
            "parameters": {
              "since": "{{last_version_tag}}",
              "until": "HEAD",
              "format": "json"
            },
            "output": "commits"
          },
          {
            "id": "categorize-changes",
            "action": "analyze_commits",
            "input": "commits",
            "rules": {
              "feat:": "Features",
              "fix:": "Bug Fixes",
              "docs:": "Documentation",
              "style:": "Styling",
              "refactor:": "Code Refactoring",
              "perf:": "Performance Improvements",
              "test:": "Tests",
              "build:": "Build System",
              "ci:": "Continuous Integration",
              "chore:": "Chores",
              "revert:": "Reverts",
              "BREAKING CHANGE:": "Breaking Changes"
            },
            "output": "categorized_changes"
          },
          {
            "id": "determine-version",
            "action": "calculate_version_bump",
            "input": "categorized_changes",
            "strategy": "semantic",
            "rules": {
              "breaking_changes": "major",
              "features": "minor",
              "fixes": "patch"
            },
            "output": "new_version"
          },
          {
            "id": "generate-markdown",
            "action": "format_changelog",
            "inputs": ["categorized_changes", "new_version"],
            "template": "changelog_template",
            "output": "changelog_content"
          }
        ]
      },

      "validation_rules": [
        "Each commit must be categorized",
        "Version bump must follow semver",
        "Changelog must include all changes",
        "Breaking changes must be highlighted"
      ]
    },

    "constraints": {
      "forbidden_behaviors": [
        "modifying_git_history",
        "executing_git_commands",
        "revealing_sensitive_information",
        "including_merge_commits"
      ],
      "required_behaviors": [
        "follow_conventional_commits",
        "maintain_chronological_order",
        "group_by_category",
        "include_commit_references"
      ],
      "quality_requirements": {
        "completeness": "all_commits_processed",
        "accuracy": "correct_categorization",
        "readability": "clear_descriptions"
      }
    },

    "output_format": {
      "type": "object",
      "properties": {
        "version": {
          "type": "string",
          "pattern": "^\\d+\\.\\d+\\.\\d+$"
        },
        "date": {
          "type": "string",
          "format": "date"
        },
        "changes": {
          "type": "object",
          "properties": {
            "breaking": { "type": "array", "items": { "type": "string" } },
            "features": { "type": "array", "items": { "type": "string" } },
            "fixes": { "type": "array", "items": { "type": "string" } },
            "other": { "type": "array", "items": { "type": "string" } }
          }
        },
        "markdown": {
          "type": "string"
        },
        "commit_count": {
          "type": "integer"
        }
      },
      "required": ["version", "date", "changes", "markdown"]
    },

    "error_handling": {
      "no_commits": {
        "action": "return_message",
        "message": "No changes since last version"
      },
      "invalid_commits": {
        "action": "skip_and_log",
        "continue": true
      },
      "git_error": {
        "action": "fail_with_context",
        "include_error": true
      }
    }
  },

  "variables": [
    {
      "name": "last_version_tag",
      "type": "string",
      "required": true,
      "description": "Git tag of the last released version",
      "validation": "^v?\\d+\\.\\d+\\.\\d+$",
      "example": "v1.2.3"
    },
    {
      "name": "repository_path",
      "type": "string",
      "required": false,
      "default": ".",
      "description": "Path to git repository",
      "validation": "^[\\w/\\-\\.]+$"
    },
    {
      "name": "include_contributors",
      "type": "boolean",
      "required": false,
      "default": true,
      "description": "Include contributor attribution"
    }
  ],

  "quality_baseline": {
    "adherence_target": 90,
    "consistency_target": 0.95,
    "performance_target": 3000
  },

  "templates": {
    "changelog_template": "## [{{version}}] - {{date}}\n\n{{#if breaking}}### âš  BREAKING CHANGES\n{{#each breaking}}- {{this}}\n{{/each}}\n{{/if}}\n\n{{#if features}}### Features\n{{#each features}}- {{this}}\n{{/each}}\n{{/if}}\n\n{{#if fixes}}### Bug Fixes\n{{#each fixes}}- {{this}}\n{{/each}}\n{{/if}}"
  }
}
```

### Example 3: Dependency Analysis Agent

```json
{
  "id": "dependency-analyzer-001",
  "version": "1.0.0",
  "name": "Dependency Analysis Agent",
  "description": "Analyzes package dependencies for security and compatibility issues",
  "category": "validation",

  "structure": {
    "role": {
      "identity": "dependency-auditor",
      "purpose": "Identify dependency issues and suggest updates",
      "capabilities": {
        "primary": [
          "package_json_parsing",
          "vulnerability_checking",
          "version_comparison",
          "compatibility_analysis"
        ],
        "constraints": [
          "no_automatic_updates",
          "read_only_analysis",
          "respect_version_constraints"
        ]
      }
    },

    "instructions": {
      "workflow": {
        "type": "parallel",
        "branches": [
          {
            "id": "security-scan",
            "steps": [
              {
                "action": "fetch_vulnerability_database",
                "source": "npm_audit_api"
              },
              {
                "action": "scan_dependencies",
                "severity_levels": ["critical", "high", "medium", "low"]
              },
              {
                "action": "generate_security_report"
              }
            ]
          },
          {
            "id": "outdated-check",
            "steps": [
              {
                "action": "fetch_latest_versions",
                "registry": "npm"
              },
              {
                "action": "compare_versions",
                "strategy": "semver"
              },
              {
                "action": "categorize_updates",
                "categories": ["major", "minor", "patch"]
              }
            ]
          },
          {
            "id": "license-audit",
            "steps": [
              {
                "action": "extract_licenses"
              },
              {
                "action": "check_compatibility",
                "project_license": "{{project_license}}"
              }
            ]
          }
        ],
        "merge_strategy": "combine_reports"
      }
    },

    "constraints": {
      "forbidden_behaviors": [
        "modifying_package_files",
        "installing_packages",
        "accessing_private_registries"
      ],
      "required_behaviors": [
        "check_all_dependencies",
        "include_transitive_deps",
        "provide_actionable_recommendations"
      ]
    },

    "output_format": {
      "type": "object",
      "properties": {
        "summary": {
          "type": "object",
          "properties": {
            "total_dependencies": { "type": "integer" },
            "vulnerabilities": {
              "type": "object",
              "properties": {
                "critical": { "type": "integer" },
                "high": { "type": "integer" },
                "medium": { "type": "integer" },
                "low": { "type": "integer" }
              }
            },
            "outdated": { "type": "integer" },
            "license_issues": { "type": "integer" }
          }
        },
        "recommendations": {
          "type": "array",
          "items": {
            "type": "object",
            "properties": {
              "package": { "type": "string" },
              "current": { "type": "string" },
              "recommended": { "type": "string" },
              "reason": { "type": "string" },
              "breaking_change": { "type": "boolean" }
            }
          }
        }
      }
    }
  },

  "variables": [
    {
      "name": "project_license",
      "type": "string",
      "required": false,
      "default": "MIT",
      "description": "Project's license for compatibility checking"
    }
  ],

  "quality_baseline": {
    "adherence_target": 92,
    "consistency_target": 0.96,
    "performance_target": 5000
  }
}
```

## XML Prompt Example (Research Agent Only)

Per ADR-002's scoped exception, the single MVP research agent can use XML-structured prompts for enhanced instruction clarity:

### Example 4: Research Agent with XML Structure

```xml
---
id: research-agent-xml-001
version: 1.0.0
name: Web Research Agent
description: Comprehensive web research agent using XML structure (MVP exception)
category: research
---

<ai_meta>
  <parsing_rules>
    - Process XML blocks first for structured data
    - Execute instructions in sequential order
    - Use templates as exact patterns
    - Request missing data rather than assuming
    - Validate each step before proceeding
  </parsing_rules>
  <file_conventions>
    - encoding: UTF-8
    - line_endings: LF
    - output_format: structured_json
  </file_conventions>
</ai_meta>

## Research Agent Configuration

<purpose>
  - Conduct comprehensive web research on specified topics
  - Synthesize information from multiple sources
  - Provide credibility-scored, fact-checked analysis
  - Generate structured research reports
</purpose>

<context>
  - Part of @orchestr8 MVP demonstration
  - Single agent allowed to use XML structure per ADR-002
  - Integrates with web search and content extraction tools
  - Produces high-quality research outputs
</context>

<constraints>
  <forbidden_behaviors>
    - Making unsupported claims
    - Using unreliable sources without disclaimer
    - Plagiarizing content
    - Accessing restricted or private information
    - Executing code or system commands
  </forbidden_behaviors>
  <required_behaviors>
    - Cite all sources with URLs
    - Score source credibility
    - Cross-reference facts across sources
    - Maintain academic integrity
    - Request clarification when ambiguous
  </required_behaviors>
  <quality_standards>
    - Minimum 3 sources per claim
    - Credibility threshold: 0.7
    - Fact verification required
    - Bias acknowledgment mandatory
  </quality_standards>
</constraints>

<process_flow>

<step number="1" name="topic_analysis">
### Step 1: Analyze Research Topic

<step_metadata>
  <inputs>
    - research_query: string
    - scope_parameters: object
  </inputs>
  <outputs>
    - search_terms: array
    - research_boundaries: object
  </outputs>
  <validation>required</validation>
</step_metadata>

<execution_logic>
  <parse_query>
    - Extract key concepts
    - Identify domain/field
    - Determine research type
  </parse_query>
  <generate_search_strategy>
    - Primary search terms
    - Alternative formulations
    - Exclusion criteria
  </generate_search_strategy>
</execution_logic>

<instructions>
  ACTION: Parse and analyze the research query
  VALIDATE: Search terms cover all aspects
  BLOCK: If query too vague or broad
  PROCEED: When search strategy defined
</instructions>
</step>

<step number="2" name="source_discovery">
### Step 2: Discover and Evaluate Sources

<step_metadata>
  <inputs>
    - search_terms: array
  </inputs>
  <outputs>
    - candidate_sources: array
    - credibility_scores: object
  </outputs>
  <max_sources>20</max_sources>
  <min_sources>5</min_sources>
</step_metadata>

<source_evaluation>
  <credibility_factors>
    - domain_authority: 0.3
    - publication_date: 0.2
    - author_expertise: 0.2
    - citation_count: 0.15
    - peer_review: 0.15
  </credibility_factors>
  <rejection_criteria>
    - Known misinformation sites
    - Credibility score < 0.5
    - No author attribution
    - Excessive advertising
  </rejection_criteria>
</source_evaluation>

<instructions>
  ACTION: Search and evaluate sources
  VALIDATE: Minimum credibility met
  BLOCK: If insufficient quality sources
  ITERATE: Until source quota reached
</instructions>
</step>

<step number="3" name="content_extraction">
### Step 3: Extract and Analyze Content

<step_metadata>
  <inputs>
    - candidate_sources: array
  </inputs>
  <outputs>
    - extracted_content: object
    - key_findings: array
  </outputs>
</step_metadata>

<extraction_rules>
  - Extract facts with context
  - Preserve source attribution
  - Identify conflicting information
  - Note confidence levels
</extraction_rules>

<instructions>
  ACTION: Extract relevant content from sources
  VALIDATE: Content relevance to query
  TRACK: Source of each fact
  IDENTIFY: Conflicts and consensus
</instructions>
</step>

<step number="4" name="synthesis">
### Step 4: Synthesize Research Findings

<step_metadata>
  <inputs>
    - extracted_content: object
    - key_findings: array
  </inputs>
  <outputs>
    - synthesized_report: object
    - confidence_metrics: object
  </outputs>
</step_metadata>

<synthesis_approach>
  <structure>
    - Executive summary
    - Key findings
    - Detailed analysis
    - Conflicting viewpoints
    - Confidence assessment
    - Source bibliography
  </structure>
  <quality_checks>
    - Fact verification across sources
    - Bias detection and notation
    - Logical consistency
    - Completeness assessment
  </quality_checks>
</synthesis_approach>

<instructions>
  ACTION: Synthesize findings into coherent report
  VALIDATE: All claims supported by sources
  INCLUDE: Confidence levels and limitations
  FORMAT: According to output schema
</instructions>
</step>

<step number="5" name="quality_review">
### Step 5: Quality Review and Validation

<step_metadata>
  <inputs>
    - synthesized_report: object
  </inputs>
  <outputs>
    - final_report: object
    - quality_score: number
  </outputs>
</step_metadata>

<review_checklist>
  - [ ] All sources cited properly
  - [ ] Claims cross-referenced
  - [ ] Confidence levels indicated
  - [ ] Limitations acknowledged
  - [ ] Bias considerations noted
  - [ ] Formatting correct
</review_checklist>

<instructions>
  ACTION: Perform final quality review
  VALIDATE: All checklist items pass
  CALCULATE: Overall quality score
  FINALIZE: Report for output
</instructions>
</step>

</process_flow>

<error_protocols>
  <insufficient_sources>
    ACTION: Expand search parameters
    FALLBACK: Report limitations clearly
    REQUEST: User guidance on scope
  </insufficient_sources>
  <conflicting_information>
    ACTION: Present all viewpoints
    WEIGHT: By source credibility
    NOTE: Conflicts explicitly
  </conflicting_information>
  <timeout_exceeded>
    ACTION: Return partial results
    INDICATE: Incomplete sections
    PROVIDE: Continuation strategy
  </timeout_exceeded>
</error_protocols>

<output_template>
{
  "research_report": {
    "id": "{{report_id}}",
    "timestamp": "{{timestamp}}",
    "query": "{{original_query}}",
    "executive_summary": "{{summary}}",
    "key_findings": [
      {
        "finding": "{{finding_text}}",
        "confidence": {{confidence_score}},
        "sources": ["{{source_1}}", "{{source_2}}"]
      }
    ],
    "detailed_analysis": {
      "sections": [
        {
          "title": "{{section_title}}",
          "content": "{{section_content}}",
          "citations": ["{{citation_1}}"]
        }
      ]
    },
    "source_bibliography": [
      {
        "url": "{{source_url}}",
        "title": "{{source_title}}",
        "credibility_score": {{score}},
        "accessed_date": "{{date}}"
      }
    ],
    "metadata": {
      "sources_analyzed": {{count}},
      "confidence_score": {{overall_confidence}},
      "processing_time": {{milliseconds}},
      "limitations": ["{{limitation_1}}"]
    }
  }
}
</output_template>

<variable_definitions>
  <variable name="research_query" type="string" required="true">
    <description>The research topic or question to investigate</description>
    <validation>Min 10 characters, max 500 characters</validation>
    <example>Impact of artificial intelligence on software development productivity</example>
  </variable>

  <variable name="scope_parameters" type="object" required="false">
    <description>Parameters to bound the research scope</description>
    <properties>
      <date_range>Last N years of publications</date_range>
      <domains>Specific domains to focus on</domains>
      <exclude>Topics or sources to exclude</exclude>
    </properties>
    <default>{ "date_range": 5, "domains": [], "exclude": [] }</default>
  </variable>

  <variable name="max_sources" type="integer" required="false">
    <description>Maximum number of sources to analyze</description>
    <validation>Between 5 and 50</validation>
    <default>20</default>
  </variable>

  <variable name="credibility_threshold" type="number" required="false">
    <description>Minimum credibility score for sources</description>
    <validation>Between 0.5 and 1.0</validation>
    <default>0.7</default>
  </variable>
</variable_definitions>

<quality_metrics>
  <adherence_target>95</adherence_target>
  <consistency_target>0.90</consistency_target>
  <performance_target>8000</performance_target>
  <measurement_approach>
    - Track instruction following per step
    - Measure output format compliance
    - Validate source quality metrics
    - Monitor performance benchmarks
  </measurement_approach>
</quality_metrics>
```

## Usage Examples

### Using a JSON Prompt

```typescript
// Load and execute a JSON prompt
const orchestrator = new Orchestrator()
const changelogAgent = await orchestrator.loadAgent('changelog-generator-001')

const result = await orchestrator.execute(changelogAgent, {
  variables: {
    last_version_tag: 'v2.1.0',
    repository_path: './my-project',
    include_contributors: true,
  },
  context: {
    execution_id: 'exec-123',
    timestamp: new Date().toISOString(),
  },
})

console.log(result.output.markdown)
```

### Using the XML Research Agent

```typescript
// The single MVP research agent using XML structure
const researchAgent = new ResearchAgent({
  template: 'research-agent-xml-001',
  format: 'xml', // Special flag for the MVP exception
})

const research = await orchestrator.execute(researchAgent, {
  variables: {
    research_query: 'Impact of quantum computing on cryptography',
    scope_parameters: {
      date_range: 3,
      domains: ['academic', 'industry'],
      exclude: ['speculation', 'science fiction'],
    },
    max_sources: 15,
    credibility_threshold: 0.75,
  },
})

// Output is still JSON despite XML template
console.log(research.output.research_report.executive_summary)
```

### Prompt Composition Example

```typescript
// Compose prompts by extending base templates
const composer = new PromptComposer()

const customChangelog = await composer.compose('changelog-generator-001', {
  extensions: [
    {
      id: 'security-focus',
      additional_instructions: {
        steps: [
          {
            action: 'highlight_security_fixes',
            priority: 'high',
          },
        ],
      },
      additional_constraints: {
        required_behaviors: ['emphasize_cve_numbers'],
      },
    },
  ],
})

// Use the composed prompt
const securityChangelog = await orchestrator.execute(customChangelog, {
  variables: { last_version_tag: 'v3.0.0' },
})
```

## Quality Validation Examples

### Adherence Score Measurement

```typescript
const scorer = new PromptAdherenceScorer()

// Score a response against its prompt
const score = await scorer.scoreResponse(
  changelogAgent.prompt,
  result.output,
  result.context,
)

console.log(`Adherence Score: ${score.overall_score}/100`)
console.log(`Categories:`, score.categories)

// Check if meets baseline
if (score.overall_score < changelogAgent.quality_baseline.adherence_target) {
  console.warn('Response below quality threshold')
}
```

### Consistency Testing

```typescript
const consistency = new ConsistencyMeasurement()

// Run agent multiple times
const responses = await Promise.all(
  Array(5)
    .fill(null)
    .map(() => orchestrator.execute(changelogAgent, { variables })),
)

// Measure consistency
const metrics = await consistency.measureConsistency(
  responses,
  changelogAgent.prompt,
)

console.log(`Semantic Similarity: ${metrics.similarity_score}`)
console.log(`Format Consistency: ${metrics.format_consistency}`)
```

## Best Practices

1. **Start with JSON** - Use JSON prompts for all agents except the research agent
2. **Version prompts** - Always increment versions when modifying prompts
3. **Test thoroughly** - Validate adherence and consistency before production
4. **Monitor quality** - Track quality metrics in production
5. **Compose wisely** - Use composition for variants, not completely new agents
6. **Document variables** - Provide clear descriptions and examples
7. **Set realistic baselines** - Start with 85% adherence, improve over time
8. **Use templates** - Leverage variable interpolation for flexibility

## Conclusion

These examples demonstrate the @orchestr8 prompt system's flexibility and power, supporting both simple demo agents and complex production workflows. The JSON-first approach (with the single XML exception) ensures consistency while the quality measurement framework guarantees reliability and continuous improvement.
